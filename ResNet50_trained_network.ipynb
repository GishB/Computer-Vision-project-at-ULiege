{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet50-trained network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpoZ48H4QIA",
        "outputId": "26d03038-4bc1-4c9b-faf1-fc8e9fa33d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "woeHWaId4pwG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import all libraries"
      ],
      "metadata": {
        "id": "qtr8ljYY43H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all libraries which is required for project tasks\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset\n",
        "from torch.types import Device\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "import matplotlib.pyplot as plt \n",
        "import sklearn.metrics #for F1 score, precision, recall\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve"
      ],
      "metadata": {
        "id": "MfgDdgj14xMH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#X-ray chest images"
      ],
      "metadata": {
        "id": "wYR3oZbR5CH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/DATASET_FINAL/images' #root which contents all images in RGB\n",
        "csv_dir = '/content/drive/MyDrive/DATASET_FINAL/x_ray_new.csv' #root to csv file which content in 0 column images name and in 1 column class"
      ],
      "metadata": {
        "id": "WUW7WF0J45Qh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "aMwAWRfY5ETH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XRayChestDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        " #   image = io.imread(img_path)\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return (image, y_label)"
      ],
      "metadata": {
        "id": "iAs1Kkdq5DsY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform rule for data\n",
        "preprocess = transforms.Compose([\n",
        "  #do size 64 to 64 to make computations faster for computer\n",
        "  transforms.Resize(size=(64, 64)),\n",
        "  #do transform.ToTensor for dataset in pytorch\n",
        "  transforms.ToTensor(),\n",
        "  transforms.RandomRotation(degrees = 5),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "\n",
        "#Dataset which we are gonna working with\n",
        "dataset = XRayChestDataset(csv_file = csv_dir, root_dir = root_dir, transform = preprocess)\n",
        "\n",
        "#split data to 70/20/10 sets\n",
        "number_of_images = len(dataset)\n",
        "\n",
        "number_of_images_train = int(number_of_images*0.7)\n",
        "number_of_images_test = int(number_of_images*0.2)\n",
        "number_of_images_valid = number_of_images - number_of_images_train - number_of_images_test\n",
        "\n",
        "train_set, test_set, valid_set = torch.utils.data.random_split(dataset, [number_of_images_train,\n",
        "                                                                         number_of_images_test,\n",
        "                                                                         number_of_images_valid])"
      ],
      "metadata": {
        "id": "qfStTps85IOF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hint: DO NOT USE JUPYNER NOTEBOOK! IT DOSENT SUPPORT multiprocess. There is a lag when num_workers > 0!\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=64, shuffle=False, num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=1, num_workers = 2) "
      ],
      "metadata": {
        "id": "PuoSZ9UD5J0t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What do we input to dataset. Just check.\n",
        "classes = ('NORMAL', 'PNEUMONIA')\n",
        "\n",
        "def show_images(img):\n",
        "    plt.imshow(transforms.functional.to_pil_image(img))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "show_images(utils.make_grid(images))  # should look weird due to normalization\n",
        "print(*[classes[l] for l in labels])\n",
        "\n",
        "#Should working about 1 minute before it shows images and labels"
      ],
      "metadata": {
        "id": "EInbutUf5OWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet upload"
      ],
      "metadata": {
        "id": "2g3WTPG_5caO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # The expansion size is always 4 for ResNet 50,101,152\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet50(img_channel=3, num_classes=2):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)"
      ],
      "metadata": {
        "id": "7Ds-oOom5VJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model_ResNet50 = ResNet50()\n",
        "model_ResNet50.load_state_dict(torch.load('/content/drive/MyDrive/DATASET_FINAL/AlexNet50_trained', map_location = device))"
      ],
      "metadata": {
        "id": "bxpawe0o5e9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check model work"
      ],
      "metadata": {
        "id": "Q8Q2lhAR5qnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    prediction_list = []\n",
        "    y_list = []\n",
        "\n",
        "    model_ResNet50.eval()\n",
        "\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model_ResNet50(x)\n",
        "\n",
        "            _, predictions = scores.max(1)\n",
        "\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "            prediction_list.append(np.array(predictions))\n",
        "            y_list.append(np.array(y))\n",
        "\n",
        "            if predictions == 1:\n",
        "              if y == 1:\n",
        "                tp += 1\n",
        "              elif y == 0:\n",
        "                fp += 1\n",
        "            if predictions == 0:\n",
        "              if y == 1:\n",
        "                fn += 1\n",
        "              elif y == 0:\n",
        "                tn += 1\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "    precision_temp = sklearn.metrics.precision_score(y_list, prediction_list)\n",
        "    recall_temp = sklearn.metrics.recall_score(y_list, prediction_list)\n",
        "    f1 = sklearn.metrics.f1_score(y_list, prediction_list)\n",
        "    \n",
        "    return np.array(num_correct/num_samples), precision_temp, recall_temp, f1, tp, tn, fp, fn"
      ],
      "metadata": {
        "id": "kBWe--FO5j_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy(valid_loader)"
      ],
      "metadata": {
        "id": "EQPyqTVG5the"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cqq5mbXy5vTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}